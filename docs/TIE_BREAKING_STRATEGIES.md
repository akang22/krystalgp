# ğŸ¯ Tie-Breaking Strategies for Conflicting Parser Results

## The Problem

When different parsers extract different EBITDA values from the same email:
- NER Body: $4.50M (from email body)
- LLM Body: $4.50M (from email body)
- Layout Vision: $3.60M (from PDF attachment)

**Which value is correct?**

---

## âœ… Current Implementation: Selection-Based (NOT Averaging)

The ensemble parser **SELECTS** the best value using this priority:

### 1ï¸âƒ£ **Fuzzy Consensus** (Primary) â­
**Strategy:** If multiple parsers return values within Â±$0.5M, treat as "same" and select that value.

**Example:**
```
Parser Results: [$4.50M, $4.50M, $3.60M]
â†’ Cluster 1: [$4.5, $4.5] (2 parsers)
â†’ Cluster 2: [$3.6] (1 parser)
â†’ SELECT: $4.50M (majority cluster)
```

**Why it's good:** Handles minor rounding differences, respects majority agreement

---

### 2ï¸âƒ£ **Majority Vote** (Exact Match)
**Strategy:** If >50% of parsers return the exact same value, select it.

**Example:**
```
Parser Results: [$5.0M, $5.0M, $5.0M, $8.0M]
â†’ $5.0M appears 3/4 times (75%)
â†’ SELECT: $5.0M
```

---

### 3ï¸âƒ£ **Confidence Selection** (Highest Score)
**Strategy:** If values are different, select from parser with highest confidence score.

**Confidence Scoring:**
```python
score = parser_weight Ã— source_weight Ã— raw_text_bonus

Parser Weights:
- LLM:    1.0  (highest reliability)
- Vision: 0.9  (high reliability, layout-aware)
- NER:    0.7  (baseline)
- OCR:    0.5  (OCR quality varies)

Source Weights:
- Attachment: 1.2Ã— (detailed documents)
- Body:       1.0Ã— (email text)
- Both:       1.1Ã—

Raw Text Bonus:
- Has raw EBITDA text: 1.1Ã—
- No raw text:         1.0Ã—
```

**Example:**
```
NER:    $4.5M â†’ score = 0.7 Ã— 1.0 Ã— 1.1 = 0.77
LLM:    $4.5M â†’ score = 1.0 Ã— 1.0 Ã— 1.1 = 1.10  â† HIGHEST
Vision: $3.6M â†’ score = 0.9 Ã— 1.2 Ã— 1.1 = 1.19  â† HIGHEST!

â†’ SELECT: $3.6M (Vision has highest score due to attachment bonus)
```

---

### 4ï¸âƒ£ **Source Prioritization**
**Strategy:** Prefer attachment-based over body-based (documents are more detailed).

**Example:**
```
LLM Body:      $4.5M (from email)
Layout Vision: $3.6M (from PDF)
â†’ SELECT: $3.6M (attachment source prioritized)
```

---

### 5ï¸âƒ£ **Historical Validation**
**Strategy:** Compare against `results.csv`, select value closest to historical data.

**Example:**
```
Parser Results: [$4.5M, $3.6M]
Historical (results.csv): $4.5M
â†’ SELECT: $4.5M (matches historical)
```

---

## ğŸ”„ Full Strategy Chain

The ensemble tries strategies in order:

```
1. Fuzzy Consensus â†’ If values are close (Â±$0.5M), pick consensus
   â†“ (if multiple distinct values)
   
2. Majority Vote â†’ If >50% agree exactly, pick that value
   â†“ (if no majority)
   
3. Confidence Selection â†’ Pick value from highest-scored parser
   â†“ (if all failed)
   
4. Source Prioritization â†’ Prefer attachment over body
   â†“ (if all failed)
   
5. Historical Validation â†’ Compare with results.csv
   â†“ (if all failed)
   
6. First Available â†’ Return first non-None value
```

---

## ğŸ“Š Real Example: Project Gravy

**Parser Results:**
- NER Body: $4.50M (body, score: 0.77)
- LLM Body: $4.50M (body, score: 1.10)
- Layout Vision: $3.60M (attachment, score: 1.19)

**Selection Process:**

âœ… **Step 1: Fuzzy Consensus**
- Values: [$4.5, $4.5, $3.6]
- Cluster 1: [$4.5, $4.5] - 2 parsers (MAJORITY)
- Cluster 2: [$3.6] - 1 parser
- **â†’ SELECT: $4.50M** âœ“

*Strategy stopped here - fuzzy consensus found majority cluster*

---

## ğŸš« What We DON'T Do

âŒ **NO Averaging:** We never compute `(4.5 + 4.5 + 3.6) / 3 = 4.2`
   - Meaningless to average different metrics
   - $4.5M "adjusted EBITDA" â‰  $3.6M "portfolio EBITDA"

âŒ **NO Blending:** Values are selected whole, not interpolated

âœ… **YES Selection:** Pick ONE value based on evidence and confidence

---

## ğŸ’¡ When Should You Use Each Strategy?

| Situation | Best Strategy | Why |
|-----------|---------------|-----|
| Parsers mostly agree | Fuzzy Consensus | Handles rounding differences |
| One parser is known reliable | Confidence Selection | Trust the best source |
| Attachments are more detailed | Source Prioritization | PDFs have more data |
| You have historical data | Historical Validation | Ground truth reference |
| High-stakes decision | Human-in-the-Loop | Manual review |

---

## ğŸ”§ Customizing Weights

Edit `src/email_parser/ensemble_parser.py`:

```python
# Adjust parser reliability weights
parser_weights = {
    'LLM': 1.0,    # â† Increase if LLM is very accurate
    'Vision': 0.9,  # â† Increase for better PDFs
    'NER': 0.7,     # â† Your baseline
    'OCR': 0.5,     # â† Decrease if OCR quality is poor
}

# Adjust source importance
source_weights = {
    'attachment': 1.2,  # â† Increase to trust PDFs more
    'body': 1.0,        # â† Email text baseline
}
```

---

## ğŸ“ˆ Validation Through Testing

Run full evaluation to see which strategy performs best:

```bash
uv run python scripts/run_evaluation.py
```

Then compare accuracy across your 99 emails to calibrate weights!

---

**Bottom line:** We **select** the most trustworthy value, we **don't blend** meaningless averages. âœ…

