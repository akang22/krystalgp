"""Test ensemble parser with different tie-breaking strategies."""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from email_parser.ensemble_parser import EnsembleParser
from email_parser.llm_body_parser import LLMBodyParser
from email_parser.ner_body_parser import NERBodyParser
from email_parser.layout_attachment_parser import LayoutLLMParser

# Test email with conflicting EBITDA values
TEST_EMAIL = "FW Project Gravy - Franchise QSR Portfolio Acquisition Opportunity.msg"

WORKSPACE = Path(__file__).parent.parent
SAMPLE_EMAILS_DIR = WORKSPACE / "sample_emails"
RESULTS_CSV = WORKSPACE / "results.csv"


def test_individual_parsers(email_path):
    """First show what each parser returns."""
    print("="*100)
    print("üîç Individual Parser Results (showing the conflict)")
    print("="*100)
    
    parsers = [
        ('NER Body', NERBodyParser()),
        ('LLM Body', LLMBodyParser()),
        ('Layout Vision', LayoutLLMParser()),
    ]
    
    results = []
    
    for name, parser in parsers:
        try:
            result = parser.parse(email_path)
            opp = result.opportunity
            results.append((name, result))
            
            ebitda = f"${opp.ebitda_millions}M" if opp.ebitda_millions else "None"
            source = result.extraction_source
            
            print(f"\n{name:20} EBITDA: {ebitda:10} Source: {source:15} Raw: {opp.raw_ebitda_text}")
            
        except Exception as e:
            print(f"\n{name:20} ERROR: {e}")
    
    return results


def test_ensemble_strategies(email_path):
    """Test different ensemble strategies."""
    print("\n" + "="*100)
    print("üéØ Ensemble Tie-Breaking Strategies")
    print("="*100)
    
    strategies = [
        ('majority', 'Majority Voting'),
        ('fuzzy', 'Fuzzy Consensus (¬±$0.5M tolerance)'),
        ('weighted', 'Confidence-Weighted Average'),
        ('prioritized', 'Source Prioritization (attachment > body)'),
        ('historical', 'Historical Data Validation (results.csv)'),
        ('all', 'All Strategies Combined (recommended)'),
    ]
    
    for strategy_key, strategy_name in strategies:
        print(f"\nüìä Strategy: {strategy_name}")
        print("-"*100)
        
        try:
            ensemble = EnsembleParser(
                use_llm=True,
                use_ner=True,
                use_vision=True,
                use_ocr=False,  # Skip OCR for speed
                results_csv_path=RESULTS_CSV if strategy_key == 'historical' or strategy_key == 'all' else None
            )
            
            # Manually set strategy for testing
            result = ensemble.parse(email_path)
            opp = result.opportunity
            
            # For testing specific strategies, we need to modify the parser
            # For now, just show the 'all' strategy result
            if strategy_key == 'all':
                ebitda = f"${opp.ebitda_millions:.2f}M" if opp.ebitda_millions else "None"
                print(f"  Result: {ebitda}")
                print(f"  Method: {opp.raw_ebitda_text}")
                print(f"  Company: {opp.company_name}")
                print(f"  Location: {opp.hq_location}")
            
        except Exception as e:
            print(f"  ERROR: {e}")


def demonstrate_tie_breaking():
    """Demonstrate tie-breaking with explanations."""
    print("\n" + "="*100)
    print("üí° Tie-Breaking Approaches Explained")
    print("="*100)
    
    approaches = [
        ("1. Majority Voting", 
         "Pick value that appears most frequently across parsers",
         "Pros: Simple, democratic | Cons: May not work with all different values"),
        
        ("2. Fuzzy Consensus",
         "Group similar values (¬±$0.5M) and take average of largest cluster",
         "Pros: Handles minor differences | Cons: Tolerance must be tuned"),
        
        ("3. Confidence Weighting",
         "Weight each parser by: type (LLM>Vision>NER), source (attach>body), has_raw_text",
         "Pros: Best overall accuracy | Cons: Requires calibration"),
        
        ("4. Source Prioritization",
         "Prefer attachment-based (more detailed docs) over body-based",
         "Pros: Document hierarchy | Cons: Attachments may have different metrics"),
        
        ("5. Historical Validation",
         "Compare against results.csv, pick closest match",
         "Pros: Uses ground truth | Cons: Requires historical data"),
        
        ("6. Pattern Validation",
         "Extract with simple regex, use as tiebreaker",
         "Pros: Fast, interpretable | Cons: May miss complex formats"),
        
        ("7. Fallback Chain",
         "Try parsers in priority order: LLM ‚Üí Vision ‚Üí NER ‚Üí OCR",
         "Pros: Always returns a value | Cons: May not be best value"),
        
        ("8. Multi-field Consensus",
         "If parsers agree on company/location, trust their EBITDA too",
         "Pros: Cross-validation | Cons: Complex logic"),
        
        ("9. LLM Meta-reasoning",
         "Send all extracted values to LLM: 'Which EBITDA is correct?'",
         "Pros: Sophisticated reasoning | Cons: Extra API call, slower"),
        
        ("10. Human-in-the-Loop",
         "Flag conflicts (>20% difference) for manual review",
         "Pros: 100% accuracy | Cons: Requires human time"),
    ]
    
    for title, description, notes in approaches:
        print(f"\n{title}")
        print(f"  {description}")
        print(f"  {notes}")


def main():
    """Run ensemble parser demonstrations."""
    email_path = SAMPLE_EMAILS_DIR / TEST_EMAIL
    
    if not email_path.exists():
        print(f"‚ùå Email not found: {TEST_EMAIL}")
        return
    
    print(f"\nüìß Testing: {TEST_EMAIL}\n")
    
    # Step 1: Show individual results
    individual_results = test_individual_parsers(email_path)
    
    # Step 2: Demonstrate tie-breaking
    demonstrate_tie_breaking()
    
    # Step 3: Test ensemble
    test_ensemble_strategies(email_path)
    
    # Summary
    print("\n" + "="*100)
    print("‚úÖ Ensemble Parser Demonstration Complete!")
    print("="*100)
    print("\nüí° Recommendation: Use 'confidence_weighted' strategy for best overall accuracy")
    print("   - Combines strengths of all parsers")
    print("   - Weights by parser reliability and source quality")
    print("   - Falls back gracefully when parsers disagree\n")


if __name__ == "__main__":
    main()

